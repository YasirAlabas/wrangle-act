{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This report aims to give some insight on how the wrangling process of the data givin in the csv file and the tsv file that was downloaded and opened along with the tweet-json text file. The data presented in these three files are concentrated around a twitter page that allows people to rate dogs, the data extracted from that page contain different information such as dog types and favorited tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So initialy we have three files, the 'twitter_archive_enhanced.csv' which a simple csv file. The 'image_predictions.tsv' file, which is a file that must be downloaded programmatically and is hosted on Udacity's servers. And finally 'tweet_json.txt' which is a file that must be created after extracting the json information from twitter API usin Tweepy library.\n",
    "\n",
    "As for the first csv file, it was downloaded manully and and opened using pandas. The second file however was a bit more tricky, it had to be downloaded and then read row by row and transformed to a list, from the list it should be transferred again into a dataframe. Unfortunatly, for the json file, Twitter did not respond for my requests to use their API(After a whole  week) so I decided to download the 'tweet_json.txt' available in the project workplace and transformed it into json and after that into a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Issues\n",
    "\n",
    "When starting to assess the data, I had to see what are the columns in each dataframe composed of and if their datatype makes sense. I noticed immediatly after opening some samples of the three dataframes that there is a considrable amount of data that is missing, so this was the first quality issue that I recorded. I also recorded multiple other quality issues related to wrongly assigned datatypes as will as other quality types as mentioned below:\n",
    "\n",
    "- Missing values in all of the tables.\n",
    "\n",
    "- Dates should be better changed from 'object' datatype into 'date' datatype.\n",
    "\n",
    "- Inconsistency with the denominator values, should be only 10.\n",
    "\n",
    "- Inconsistency with the number of decimals of p1_conf, p2_conf and p3_conf.\n",
    "\n",
    "- Problems when extracting the numenator values with decimals in them.\n",
    "\n",
    "- Retweeted ratings should be removed.\n",
    "\n",
    "- tweet_id in twitter_df should be an 'integer' rather than an 'object'.\n",
    "\n",
    "- Multiple values in the name column of tweet_id have 'a' or 'an' as names.\n",
    "\n",
    "## Tideness Issues\n",
    "\n",
    "As for tideness issues, there weren't much of them to talk about, the main problems were that in the twitter_df, the dog types were better off being concatenated into one column. The other problem was that the data was spread between three tables, it would be easier to extract the data if they were in one big table. Here are the issues as a summary:\n",
    "\n",
    "- All the tables should be joined together to easily visulaize insights.\n",
    "\n",
    "- The columns of dog types should be concatinated into one column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When cleaning the data, I decided to go with the quality issues first so that when I try to solve the tideness issues it would be more easier to manage, since I might decide to change one of my quality solutions in the future, I also copyed all the three dataframes and made my work revolve around the three copyed dataframes, so that if a mistake was made in the cleaning segment, it won't effect the original data. I first started with the more easier issues, such as changing the datatypes to the more challanging issues such as concatenating the four dog type columns. Each issue would go through a cycle of defining, coding and testing the solution. For me the most tricky part in the cleaning segment was the concatenation of the four dog columns. I had to go through four steps:\n",
    "\n",
    "1- We need to change all the NaN variables into empty strings.\n",
    "\n",
    "2- Next, we will need to concat the four columns.\n",
    "\n",
    "3- After that, we will reverse the remaining empty strings into NaN variables.\n",
    "\n",
    "4- Finally, we will modify some of the concated variables to become more readable.\n",
    "\n",
    "One of the issues that literally drove me crazy was with the numenator changing, I tried multiple ways of changing the value of the wrongly extracted numenators to no avail, let me give an example of all the methods that I tried:\n",
    "\n",
    "#twitter_clean['rating_numerator'] = twitter_clean['rating_numerator'][twitter_clean['tweet_id']==45].replace(5, 13.5)\n",
    "#twitter_clean['name'] = twitter_clean['name'].replace(['a', 'an'], np.nan)\n",
    "#twitter_clean.iloc[45]=twitter_clean['rating_numerator'].replace(5, 13.5)\n",
    "#twitter_clean.loc[(twitter_clean[tweet_id]==883482846933004000) & (twitter_clean[rating_numerator]==5)] = 883482846933004000,13.5\n",
    "#twitter_clean.loc[twitter_clean.tweet_id == 883482846933004000, \"rating_numerator\"] = 13.5\n",
    "#twitter_clean.loc[twitter_clean['tweet_id'] == 883482846933004000, ['rating_numerator']] = 13.5\n",
    "#twitter_clean['rating_numerator'] = twitter_clean[twitter_clean['tweet_id']==883482846933004000].replace([5], 13.5)\n",
    "#twitter_clean= twitter_clean.loc[twitter_clean['tweet_id']==883482846933004000, ['rating_numerator']] = 13.5\n",
    "#twitter_clean.iloc[883482846933004000, 5.0] = 13.5\n",
    "#twitter_clean.loc[twitter_clean['tweet_id']==883482846933004000, 'rating_numerator']\n",
    "#twitter_clean.iloc[45]['rating_numerator']=13.5\n",
    "\n",
    "In the end, it turned out that I had to cast float to 13.5: float(13.5).\n",
    "\n",
    "After finishing with the data cleaning, I stored the new merged dataframe in a csv file called 'twitter_archive_master.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'wrangle_report.ipynb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
